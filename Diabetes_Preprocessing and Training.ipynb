{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Training Data Development\n",
    "\n",
    "In this notebook we will perform preprocessing and training data development for the project. The goal of this process is to standardize our model development dataset. It includes encoding categorical variables, scaling numeric features and splitting the dataset into train and test sets. Since we have already performed data encoding in the previous step (EDA), we don't have to do it again. There is also no need to perform scaling because our features are categorical (except 'age' column, which is numeric).\n",
    "\n",
    "We saw in the previous steps that our dataset consists mostly of binary features. We also determined that the problem we're trying to solve is a classification problem - classifying patients as 'positive' (have diabetes) or 'negative' (does not have diabetes). In this step we will build and train a baseline model - in this case it will be Logistic Regression model, which is good for classification problems. We will also check another model - Random Forest - in order to compare it with Logistic Regression model and see which of them perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import all necessary libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\Tutorials\\SDST\\My Projects\\Capstone2\\EDA\\Diabetes_EDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Polyuria</th>\n",
       "      <th>Polydipsia</th>\n",
       "      <th>sudden weight loss</th>\n",
       "      <th>weakness</th>\n",
       "      <th>Polyphagia</th>\n",
       "      <th>Genital thrush</th>\n",
       "      <th>visual blurring</th>\n",
       "      <th>Itching</th>\n",
       "      <th>Irritability</th>\n",
       "      <th>delayed healing</th>\n",
       "      <th>partial paresis</th>\n",
       "      <th>muscle stiffness</th>\n",
       "      <th>Alopecia</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  Polyuria  Polydipsia  sudden weight loss  weakness  \\\n",
       "0   40       0         0           1                   0         1   \n",
       "1   58       0         0           0                   0         1   \n",
       "2   41       0         1           0                   0         1   \n",
       "3   45       0         0           0                   1         1   \n",
       "4   60       0         1           1                   1         1   \n",
       "\n",
       "   Polyphagia  Genital thrush  visual blurring  Itching  Irritability  \\\n",
       "0           0               0                0        1             0   \n",
       "1           0               0                1        0             0   \n",
       "2           1               0                0        1             0   \n",
       "3           1               1                0        1             0   \n",
       "4           1               0                1        1             1   \n",
       "\n",
       "   delayed healing  partial paresis  muscle stiffness  Alopecia  Obesity  \\\n",
       "0                1                0                 1         1        1   \n",
       "1                0                1                 0         1        0   \n",
       "2                1                0                 1         1        0   \n",
       "3                1                0                 0         0        0   \n",
       "4                1                1                 1         1        1   \n",
       "\n",
       "   class  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our EDA we know that there are 320 positive cases (61.5%) and 200 negative cases (38.5%) in the dataset.\n",
    "We are going to perform train test split. We will have 20% of our data in the test set and 80% in the train set. Let's check the shape of our dataset and expected shapes of train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416.0, 104.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df) * 0.8, len(df)*0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have 416 observations in our train set and 104 observations in our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create two variables - X and y. X will be a matrix of 520 rows and 16 columns (all columns that represent a patient's characteristics) and y is going to be our target variable - a vector with a shape of (520,) (representing 'positive' or 'negative' class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(520, 16)\n",
      "(520,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform the split using scikitlearn train_test_split function. We will use 'stratify' parameter in order to keep the same proportions of 'positive' and 'negative' cases as in our dataset before the split. Since we have one feature which is non-binary - 'age' - we will need to perform scaling in order to normalize our features whitin particular range. In this case we will use StandardScaler. We will also save our splits as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitpath = '../Splits'\n",
    "if not os.path.exists(splitpath):\n",
    "    os.mkdir(splitpath)\n",
    "split_path = os.path.join(splitpath, 'Train_Test_Split.pkl')\n",
    "if not os.path.exists(split_path):\n",
    "    with open(split_path, 'wb') as sp:\n",
    "        pickle.dump(train_test_split(X, y, test_size=0.2, random_state=42, stratify=y), sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we got our train/test proportions right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 16)\n",
      "(416,)\n",
      "(104, 16)\n",
      "(104,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set has 416 observations and test set has 104 observations, just as we expected. Let's see what are the proportions of positive vs negative cases in train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    256\n",
      "0    160\n",
      "Name: class, dtype: int64\n",
      "61.53846153846154\n",
      "38.46153846153847\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print((256/416) * 100)\n",
    "print((160/416) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    64\n",
      "0    40\n",
      "Name: class, dtype: int64\n",
      "61.53846153846154\n",
      "38.46153846153847\n"
     ]
    }
   ],
   "source": [
    "print(y_test.value_counts())\n",
    "print((64/104) * 100)\n",
    "print((40/104) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in both train and test splits we have 61.5% of positive cases and 38.5% of negative cases, just as it was in our original dataset before we performed the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our baseline model. We will use logistic regression for our classification problem. First, we will not tune its hyperparameters and see what the results will be. Then, if needed, we will perform regularization parameter tuning and compare results of those two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SPLIT ACCURACY:  0.9423076923076923\n",
      "TEST SPLIT ACCURACY:  0.9423076923076923\n"
     ]
    }
   ],
   "source": [
    "clf_noparams = LogisticRegression()\n",
    "clf_noparams.fit(X_train_norm, y_train)\n",
    "clf_noparams_train_ypred = clf_noparams.predict(X_train_norm)\n",
    "clf_noparams_test_ypred = clf_noparams.predict(X_test_norm)\n",
    "print('TRAIN SPLIT ACCURACY: ', accuracy_score(y_train, clf_noparams_train_ypred))\n",
    "print('TEST SPLIT ACCURACY: ', accuracy_score(y_test, clf_noparams_test_ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN SPLIT Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       160\n",
      "           1       0.95      0.95      0.95       256\n",
      "\n",
      "    accuracy                           0.94       416\n",
      "   macro avg       0.94      0.94      0.94       416\n",
      "weighted avg       0.94      0.94      0.94       416\n",
      "\n",
      "=== TEST SPLIT Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93        40\n",
      "           1       0.98      0.92      0.95        64\n",
      "\n",
      "    accuracy                           0.94       104\n",
      "   macro avg       0.93      0.95      0.94       104\n",
      "weighted avg       0.95      0.94      0.94       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN SPLIT Classification Report ===\")\n",
    "print(classification_report(y_train, clf_noparams_train_ypred))\n",
    "print(\"=== TEST SPLIT Classification Report ===\")\n",
    "print(classification_report(y_test, clf_noparams_test_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, from comparing our train and test splits, we notice that the model showed the same accuracy on both splits. We also do not see any significant gaps between train and test split precision, recall and f1-score. Therefore we can assume that our Logistic Regression model shows good results and do not over-/underfit, even without hyperparameters tuning. However, we will still explore how the model will perform if we tune its regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a list with C parameter values and use GridSearchCV in order to find the best C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_params = LogisticRegression()\n",
    "C_params = [0.001, 0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find what regularization parameter is the best for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])\n"
     ]
    }
   ],
   "source": [
    "print(clf_params.get_params().keys())\n",
    "c = [c for c in C_params]\n",
    "grid_params = {'C':c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid = GridSearchCV(clf_params, param_grid = grid_params, cv=5, n_jobs=-1)\n",
    "clf_grid.fit(X_train_norm, y_train)\n",
    "clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing GridSearchCV we can conclude that the best regularization parameter ('C') for our classifier is C=1 (which is a default value for this parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9423076923076923\n"
     ]
    }
   ],
   "source": [
    "clf_params = LogisticRegression(C=1.0, random_state=42)\n",
    "clf_params.fit(X_train_norm, y_train)\n",
    "clfparams_ypred_train = clf_params.predict(X_train_norm)\n",
    "print(accuracy_score(clfparams_ypred_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for our model performance on the training set is 0.94."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what are the probabilities for each target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.04590655e-02 9.79540935e-01]\n",
      " [9.73672053e-03 9.90263279e-01]\n",
      " [1.34293557e-03 9.98657064e-01]\n",
      " [4.94047431e-04 9.99505953e-01]\n",
      " [9.26693923e-01 7.33060769e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(clf_params.predict_proba(X_train_norm)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use classification report on our train and test datasets in order to determine if our model overfit/underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       160\n",
      "           1       0.95      0.95      0.95       256\n",
      "\n",
      "    accuracy                           0.94       416\n",
      "   macro avg       0.94      0.94      0.94       416\n",
      "weighted avg       0.94      0.94      0.94       416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_train, clfparams_ypred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this classification report we can infer that in our train data: 1) Among 256 people which were classified as 1 ('Positive') 95% were classified correctly. 2) Among 160 people which were classified as 0 ('Negative') 93% were classified correctly. 3) Among all people who actually were 1 ('Positive') 95% were classified correctly. 4) Among all people who actually were 0 ('Negative') 93% were classified correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the performance of our classifier on training set seems to be good. Let us now check it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9423076923076923\n"
     ]
    }
   ],
   "source": [
    "clfparams_ypred_test = clf_params.predict(X_test_norm)\n",
    "print(accuracy_score(clfparams_ypred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for our model performance on the test set is 0.43, which is the same as on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71951361 0.28048639]\n",
      " [0.02096393 0.97903607]\n",
      " [0.00290846 0.99709154]\n",
      " [0.68692988 0.31307012]\n",
      " [0.95044974 0.04955026]]\n"
     ]
    }
   ],
   "source": [
    "print(clf_params.predict_proba(X_test_norm)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93        40\n",
      "           1       0.98      0.92      0.95        64\n",
      "\n",
      "    accuracy                           0.94       104\n",
      "   macro avg       0.93      0.95      0.94       104\n",
      "weighted avg       0.95      0.94      0.94       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, clfparams_ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this classification report we can infer that in our train data: 1) Among 64 people which were classified as 1 ('Positive') 98% were classified correctly (against 95% on the train data). 2) Among 40 people which were classified as 0 ('Negative') 89% were classified correctly (against 93% on the train data). 3) Among all people who actually were 1 ('Positive') 92% were classified correctly (against 95% on the train data). 4) Among all people who actually were 0 ('Negative') 97% were classified correctly (against 93% on the train data).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, there are no significant gaps between model performance on train and test sets. From this we can conclude that our Logistic Regression model does not overfit or underfit and is able to generalize on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build another model - random forest - in order to compare its performance with logistic regression model we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the model\n",
    "rfc = RandomForestClassifier(oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will fit the model without tuning its parameters and see how it's going to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY SCORE:  0.9903846153846154\n",
      "Out-of-bag SCORE:  0.9735576923076923\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_train_norm, y_train)\n",
    "rfcy_pred_np = rfc.predict(X_test_norm)\n",
    "print('ACCURACY SCORE: ', accuracy_score(rfcy_pred_np, y_test))\n",
    "print('Out-of-bag SCORE: ', rfc.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        40\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           0.99       104\n",
      "   macro avg       0.99      0.99      0.99       104\n",
      "weighted avg       0.99      0.99      0.99       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, rfcy_pred_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an accuracy score of 0.99 which is pretty good for a model with no parameters tuned. Now we will use RandomizedSearchCV in order to find what parameters are the best for this model. We are particularly interested in 'n_estimators', 'max_features' and 'max_depth'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of parameters\n",
    "rfc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of trees\n",
    "n_estimators = [int(i) for i in np.linspace(200, 2000, 10)]\n",
    "\n",
    "#number of features for each split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "#maximal depth\n",
    "max_depth = [int(i) for i in np.linspace(100, 500, 11)]\n",
    "\n",
    "#random grid\n",
    "random_grid = {'n_estimators':n_estimators, 'max_features':max_features, 'max_depth':max_depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 600, 'max_features': 'sqrt', 'max_depth': 340}\n"
     ]
    }
   ],
   "source": [
    "#randomized search\n",
    "rfc_random = RandomizedSearchCV(estimator=rfc, param_distributions=random_grid, n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "#fitting the model\n",
    "rfc_random.fit(X_train_norm, y_train)\n",
    "\n",
    "print(rfc_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV showed that the best parameters for our RandomForestClassifier are 'n_estimators':600, 'max_features':'sqrt' and 'max_depth':340. Now let's plug those parameters into the model and see if it improves its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY SCORE:  0.9807692307692307\n",
      "Out-of-bag SCORE:  0.9735576923076923\n"
     ]
    }
   ],
   "source": [
    "rfc_params = RandomForestClassifier(n_estimators=600, max_features='sqrt', max_depth=340, oob_score=True)\n",
    "rfc_params.fit(X_train_norm, y_train)\n",
    "rfcy_pred_params = rfc_params.predict(X_test_norm)\n",
    "print('ACCURACY SCORE: ', accuracy_score(rfcy_pred_params, y_test))\n",
    "print('Out-of-bag SCORE: ', rfc_params.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score of the model with tuned parameters is 0.98 (against 0.99 with no parameters tuning).The OOB-score is 0.97 (against 0.97 with no parameters tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RANDOM FOREST TEST SET Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        40\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           0.98       104\n",
      "   macro avg       0.98      0.98      0.98       104\n",
      "weighted avg       0.98      0.98      0.98       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== RANDOM FOREST TEST SET Classification Report ===\")\n",
    "print(classification_report(y_test, rfcy_pred_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOGISTIC REGRESSION TEST SET Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93        40\n",
      "           1       0.98      0.92      0.95        64\n",
      "\n",
      "    accuracy                           0.94       104\n",
      "   macro avg       0.93      0.95      0.94       104\n",
      "weighted avg       0.95      0.94      0.94       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== LOGISTIC REGRESSION TEST SET Classification Report ===\")\n",
    "print(classification_report(y_test, clfparams_ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude from the classification report that our RandomForestClassifier has better performance with its parameters tuned. Moreover, it shows better resutls than our baseline LogisticRegression model, outperforming it in every metric we used. It has higer precision, recall, f1-score and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have conducted Preprocessing and Training data development. In this notebook we built two models - LogisticRegression and RandomForestClassifier - and compared their performance. We stratified our train and test splits so they have the same proportion of 'positive' and 'negative' cases. After splitting the data we built LogisticRegression model and evaluated it performance. The model showed no significant gaps on train and test sets, indicating there were no over-/underfitting. Then we build RandomForestClassifier model and evaluated its performance on test set. This model outperformed out baseline model and showed better precision, recall, f1-score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataset\n",
    "datapath = 'D://Tutorials/SDST/My Projects/Capstone2/Preprocessing and Training'\n",
    "if not os.path.exists(datapath):\n",
    "    os.mkdir(datapath)\n",
    "datapath_Prep_Train = os.path.join(datapath, 'Diabetes_Prep_Train.csv')\n",
    "if not os.path.exists(datapath_Prep_Train):\n",
    "    df.to_csv(datapath_Prep_Train, index=False)\n",
    "\n",
    "modelpath = '../Models'\n",
    "if not os.path.exists(modelpath):\n",
    "    os.mkdir(modelpath)\n",
    "#saving random forest model\n",
    "diabetes_rfc_path = os.path.join(modelpath, 'Diabetes_RandFor.pkl', )\n",
    "if not os.path.exists(diabetes_rfc_path):\n",
    "    with open(diabetes_rfc_path, 'wb') as f:\n",
    "        pickle.dump(rfc_params, f)\n",
    "#saving logistic regression model\n",
    "diabetes_lr_path = os.path.join(modelpath, 'Diabetes_LogReg.pkl')\n",
    "if not os.path.exists(diabetes_lr_path):\n",
    "    with open(diabetes_lr_path, 'wb') as f:\n",
    "        pickle.dump(clf_params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
