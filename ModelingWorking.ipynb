{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 2 Modeling\n",
    "\n",
    "In this notebook we will perform modeling step of the data science method. The goal of this step is to develop a final model that effectively predicts our patients' class - 'positive'(1) or 'negative'(0). In the previous step we have already built two models - LogisticRegressionCassifier (which was out baseline model) and RandomForestClassifier. We assesed the performance of each of these models first without tuning and than with hyperparameter tuning using GridSearchCV for LogisticRegression and RandomizedSearchCV for RandomForest and obtained results using classification report. We noticed, that both models performed better with hyperparameter tuning, and RandomForest showed significantly better results in precision, recall and f1-score than LogisticRegression. We also saved our train_test_split as well as both our models as pickle files.\n",
    "\n",
    "In this step we will built two other models - GradientBoostingClassifier and AdaBoostClassifer, use hyperparameter tuning in order to enhance their performance and compare their results with the results of two above mentioned models that we built durint preprocessing and trainin data development step. \n",
    "\n",
    "We will use standard metrics in order to asses our classification models performance - accuracy score, precision, recall and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our encoded dataset, train_test_split and two models - RandomForest and LogisticRegression - that we built on preprocessing and training data development step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../EDA/Diabetes_EDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Polyuria</th>\n",
       "      <th>Polydipsia</th>\n",
       "      <th>sudden weight loss</th>\n",
       "      <th>weakness</th>\n",
       "      <th>Polyphagia</th>\n",
       "      <th>Genital thrush</th>\n",
       "      <th>visual blurring</th>\n",
       "      <th>Itching</th>\n",
       "      <th>Irritability</th>\n",
       "      <th>delayed healing</th>\n",
       "      <th>partial paresis</th>\n",
       "      <th>muscle stiffness</th>\n",
       "      <th>Alopecia</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  Polyuria  Polydipsia  sudden weight loss  weakness  \\\n",
       "0   40       0         0           1                   0         1   \n",
       "1   58       0         0           0                   0         1   \n",
       "2   41       0         1           0                   0         1   \n",
       "3   45       0         0           0                   1         1   \n",
       "4   60       0         1           1                   1         1   \n",
       "\n",
       "   Polyphagia  Genital thrush  visual blurring  Itching  Irritability  \\\n",
       "0           0               0                0        1             0   \n",
       "1           0               0                1        0             0   \n",
       "2           1               0                0        1             0   \n",
       "3           1               1                0        1             0   \n",
       "4           1               0                1        1             1   \n",
       "\n",
       "   delayed healing  partial paresis  muscle stiffness  Alopecia  Obesity  \\\n",
       "0                1                0                 1         1        1   \n",
       "1                0                1                 0         1        0   \n",
       "2                1                0                 1         1        0   \n",
       "3                1                0                 0         0        0   \n",
       "4                1                1                 1         1        1   \n",
       "\n",
       "   class  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train_test_split\n",
    "with open('../Splits/Train_Test_Split.pkl', 'rb') as tts_pickle:\n",
    "    tts = pickle.load(tts_pickle)\n",
    "#loading RandomForestClassifier\n",
    "with open('../Models/Diabetes_RandFor.pkl', 'rb') as rfc_pickle:\n",
    "    rfc = pickle.load(rfc_pickle)\n",
    "#loading LogisticRegression\n",
    "with open('../Models/Diabetes_LogReg.pkl', 'rb') as lr_pickle:\n",
    "    lr = pickle.load(lr_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_norm = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=340, max_features='sqrt', n_estimators=600,\n",
       "                       oob_score=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_ypred = rfc.predict(X_test_norm)\n",
    "lr_ypred = lr.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see classification reports for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== RANDOM FOREST CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        40\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           0.98       104\n",
      "   macro avg       0.98      0.98      0.98       104\n",
      "weighted avg       0.98      0.98      0.98       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('====== RANDOM FOREST CLASSIFICATION REPORT =====')\n",
    "print(classification_report(y_test, rfc_ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== LOGISTIC REGRESSION CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83        40\n",
      "           1       1.00      0.75      0.86        64\n",
      "\n",
      "    accuracy                           0.85       104\n",
      "   macro avg       0.86      0.88      0.85       104\n",
      "weighted avg       0.89      0.85      0.85       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('====== LOGISTIC REGRESSION CLASSIFICATION REPORT =====')\n",
    "print(classification_report(y_test, lr_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from the previous step, our RandomForestClassifier showed significantly better results than LogisticRegression model. Now we will build two more models. We will start with GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize our model, fit it to the training set and see how it performs. We will not use any hyperparameter tuning now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "gbc.fit(X_train_norm, y_train)\n",
    "gbc_ypred = gbc.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== GBC ACCURACY SCORE =====\n",
      "0.9903846153846154\n",
      "===== GBC CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        40\n",
      "           1       1.00      0.98      0.99        64\n",
      "\n",
      "    accuracy                           0.99       104\n",
      "   macro avg       0.99      0.99      0.99       104\n",
      "weighted avg       0.99      0.99      0.99       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===== GBC ACCURACY SCORE =====')\n",
    "print(accuracy_score(y_test, gbc_ypred))\n",
    "print('===== GBC CLASSIFICATION REPORT =====')\n",
    "print(classification_report(y_test, gbc_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that even generic model with no hyperparameters tuning is able to generalize on new data and show great results. However, we will still perform parameters tuning just for the sake of comparing models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ccp_alpha', 'criterion', 'init', 'learning_rate', 'loss', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_iter_no_change', 'presort', 'random_state', 'subsample', 'tol', 'validation_fraction', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, GradientBoostingClassifier's parameters are very similar to RandomForestClassifier parameters. For our goal we are mostly interested in n_estimators, learning_rate, max_features and max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of trees\n",
    "n_estimators = [int(i) for i in np.linspace(200, 2000, 10)]\n",
    "\n",
    "#learning rates\n",
    "learning_rate = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "#number of features for each split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "#maximal depth\n",
    "max_depth = [int(i) for i in np.linspace(100, 500, 11)]\n",
    "\n",
    "#parameters grid\n",
    "param_grid = {'n_estimators':n_estimators, 'learning_rate':learning_rate, 'max_features':max_features, 'max_depth':max_depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbc_rand = RandomizedSearchCV(estimator=gbc, param_distributions=param_grid, n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
    "# gbc_rand.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gbc_rand.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the best parameters for our GradientBoostingClassifier. Now let's plug those parameters into our model and asses its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_params = GradientBoostingClassifier(n_estimators=200, learning_rate = 0.05, max_features='sqrt', max_depth = 500, random_state = 42)\n",
    "gbc_params.fit(X_train_norm, y_train)\n",
    "gbc_params_ypred = gbc_params.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== GBC_PARAMS TEST SET ACCURACY SCORE =====\n",
      "0.9903846153846154\n",
      "===== GBC_PARAMS TEST SET CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        40\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           0.99       104\n",
      "   macro avg       0.99      0.99      0.99       104\n",
      "weighted avg       0.99      0.99      0.99       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===== GBC_PARAMS TEST SET ACCURACY SCORE =====')\n",
    "print(accuracy_score(y_test, gbc_params_ypred))\n",
    "print('===== GBC_PARAMS TEST SET CLASSIFICATION REPORT =====')\n",
    "print(classification_report(y_test, gbc_params_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the classification report we can see that tuning the model's parameters did not have any significant effect on the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to build AdaBoost model and see how it's going to perform. Again, we will not tune hyperparameters first and will check how the model performs on train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ABC ACCURACY SCORE =====\n",
      "0.9519230769230769\n",
      "===== ABC CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94        40\n",
      "           1       1.00      0.92      0.96        64\n",
      "\n",
      "    accuracy                           0.95       104\n",
      "   macro avg       0.94      0.96      0.95       104\n",
      "weighted avg       0.96      0.95      0.95       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(random_state=42)\n",
    "abc.fit(X_train_norm, y_train)\n",
    "abc_ypred = abc.predict(X_test_norm)\n",
    "print('===== ABC ACCURACY SCORE =====')\n",
    "print(accuracy_score(y_test, abc_ypred))\n",
    "print('===== ABC CLASSIFICATION REPORT =====')\n",
    "print(classification_report(y_test, abc_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tune hyperparameters of AdaBoostingClassifier and check if it's goint to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['algorithm', 'base_estimator', 'learning_rate', 'n_estimators', 'random_state'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"learning_rate\": [0.05, 0.1, 0.25, 0.5, 0.75, 1],\n",
    "              \"n_estimators\": [int(i) for i in np.linspace(200, 2000, 10)]\n",
    "             }\n",
    "\n",
    "\n",
    "weak_c = DecisionTreeClassifier(random_state = 42, max_features = \"auto\", class_weight = \"balanced\", max_depth = 3)\n",
    "\n",
    "ABC = AdaBoostClassifier(base_estimator = weak_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc_rand = RandomizedSearchCV(estimator=ABC, param_distributions=param_grid, n_iter=40, cv=5, scoring='roc_auc', random_state=42)\n",
    "# abc_rand.fit(X_train_norm, y_train)\n",
    "# print(abc_rand.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plug those parameters into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_est = DecisionTreeClassifier(random_state=42, max_features='auto', class_weight='balanced', max_depth=3, splitter='best', criterion='gini')\n",
    "abc_params = AdaBoostClassifier(base_estimator=base_est, n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "abc_params.fit(X_train_norm, y_train)\n",
    "abc_params_ypred = abc_params.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ABC_PARAMS ACCURACY SCORE =====\n",
      "1.0\n",
      "===== ABC_PARAMS CLASSIFICATION REPORT =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00       104\n",
      "   macro avg       1.00      1.00      1.00       104\n",
      "weighted avg       1.00      1.00      1.00       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===== ABC_PARAMS ACCURACY SCORE =====')\n",
    "print(accuracy_score(y_test, abc_params_ypred))\n",
    "print('===== ABC_PARAMS CLASSIFICATION REPORT =====')\n",
    "print(classification_report(y_test, abc_params_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that hyperparameters tuning on ADABoostClassifier allowed us to achieve perfect score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among all models that we built, AdaBoostClassifier showed the highest results - 100% on the test set after hyperparameters tuning. This is the best models we've built so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our third model - SupportVectorMachine. For this problem we will use Radial Basis Function kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmclf = svm.SVC(kernel='rbf')\n",
    "svmclf.fit(X_train_norm, y_train)\n",
    "svmclf_ypred = svmclf.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====SVMCLF ACCURACY SCORE=====\n",
      "0.9807692307692307\n",
      "=====SVMCLF CLASSIFICATION REPORT=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        40\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           0.98       104\n",
      "   macro avg       0.98      0.98      0.98       104\n",
      "weighted avg       0.98      0.98      0.98       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('=====SVMCLF ACCURACY SCORE=====')\n",
    "print(accuracy_score(y_test, svmclf_ypred))\n",
    "print('=====SVMCLF CLASSIFICATION REPORT=====')\n",
    "print(classification_report(y_test, svmclf_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the classification report we can tell that support vector machine with RBF kernel and no hyperparameters tuning shows results that are just slightly worse than those of GradientBoostingClassifier. However, it is still inferior to ADABoostClassifier. Now let's find out the best parameters for our SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'break_ties', 'cache_size', 'class_weight', 'coef0', 'decision_function_shape', 'degree', 'gamma', 'kernel', 'max_iter', 'probability', 'random_state', 'shrinking', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmclf.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our SVM with RBF kernel we are mostly interested in two parameters (except 'kernel' itself, of course) - 'C' and 'gamma'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_param = np.logspace(-2, 10, 13)\n",
    "# gamma_param = np.logspace(-9, 3, 13)\n",
    "# grid_param = {'C':C_param, 'gamma':gamma_param}\n",
    "# svmclf_grid = GridSearchCV(svmclf, param_grid=grid_param, cv=5)\n",
    "# svmclf_grid.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(svmclf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plug those parameters into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====SVMCLF_PARAMS ACCURACY SCORE=====\n",
      "0.9807692307692307\n",
      "=====SVMCLF_PARAMS CLASSIFICATION REPORT=====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        40\n",
      "           1       0.98      0.98      0.98        64\n",
      "\n",
      "    accuracy                           0.98       104\n",
      "   macro avg       0.98      0.98      0.98       104\n",
      "weighted avg       0.98      0.98      0.98       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmclf_params = svm.SVC(kernel='rbf', C=10.0, gamma=0.1)\n",
    "svmclf_params.fit(X_train_norm, y_train)\n",
    "svmclf_params_ypred = svmclf_params.predict(X_test_norm)\n",
    "print('=====SVMCLF_PARAMS ACCURACY SCORE=====')\n",
    "print(accuracy_score(y_test, svmclf_params_ypred))\n",
    "print('=====SVMCLF_PARAMS CLASSIFICATION REPORT=====')\n",
    "print(classification_report(y_test, svmclf_params_ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that hyperparameters tuning did not change the model's performance. We can assume that those parameters we determined using GridSearchCV were used as default parameters in our first SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put our models'results into one table in order to better understand how they performed with respect to each other. We will only use tuned models since they showed better performance than not tuned ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| MODEL | ACCURACY | PRECISION (1) | PRECISION (0) | RECALL (1) | RECALL (0) | F1-SCORE (1) | F1-SCORE (0) |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| LogisticRegression | 0.85 | 1.00 | 0.71 | 0.75 | 1.00 | 0.86 | 0.83 |\n",
    "| RandomForest | 0.98 | 0.98 | 0.97 | 0.98 | 0.97 | 0.98 | 0.97 |\n",
    "| GradientBoosting | 0.90 | 0.98 | 1.00 | 1.00 | 0.97 | 0.99 | 0.99 |\n",
    "| ADABoost | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 | 1.00 |\n",
    "| Support Vector Machine | 0.98 | 0.98 | 0.97 | 0.98 | 0.97 | 0.98 | 0.97 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see from the table above, ADABoostClassifier showed the best - in fact, perfect - scores. In our future work we will use this model.\n",
    "##### It is important to point out that most of our models showed very good results. Putting our model tuning aside, there is one thing that might have contributed to those results - the dataset might have been artificially composed the way that makes it eaiser to build models with high accuracy (in generic meaning of this word) scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's refresh some moments from the EDA. We saved the correlation dataframes as pickle files. Now let's open them. Here are top5 feature-to-target and feature-to-feature correlations in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-f92759c966a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\Tutorials\\SDST\\My Projects\\Capstone2\\Correlations\\Feature-to-Target.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_to_t_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mf_to_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_to_t_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\Tutorials\\SDST\\My Projects\\Capstone2\\Correlations\\Feature-to-Feature.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_to_f_pickle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mf_to_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_to_f_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open('D:\\Tutorials\\SDST\\My Projects\\Capstone2\\Correlations\\Feature-to-Target.pkl', 'rb') as f_to_t_pickle:\n",
    "    f_to_t = pickle.load(f_to_t_pickle)\n",
    "with open('D:\\Tutorials\\SDST\\My Projects\\Capstone2\\Correlations\\Feature-to-Feature.pkl', 'rb') as f_to_f_pickle:\n",
    "    f_to_f = pickle.load(f_to_f_pickle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
